import torch
from torch import nn
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from torch.nn import functional as F
from torchvision.datasets import MNIST
from torchvision import transforms
import os

class LightningMNISTClassifier(pl.LightningModule):

    def __init__(self, epsilon=0.1):
        super().__init__()

        # mnist images are (1, 28, 28) (channels, width, height)
        self.layer_1 = torch.nn.Linear(28 * 28, 128)
        self.layer_2 = torch.nn.Linear(128, 256)
        self.layer_3 = torch.nn.Linear(256, 10)
        self.epsilon = epsilon  # Perturbation scale for FGSM

    def forward(self, x):
        batch_size, channels, width, height = x.size()

        # (b, 1, 28, 28) -> (b, 1*28*28)
        x = x.view(batch_size, -1)

        # layer 1 (b, 1*28*28) -> (b, 128)
        x = self.layer_1(x)
        x = torch.relu(x)

        # layer 2 (b, 128) -> (b, 256)
        x = self.layer_2(x)
        x = torch.relu(x)

        # layer 3 (b, 256) -> (b, 10)
        x = self.layer_3(x)

        # probability distribution over labels
        x = torch.log_softmax(x, dim=1)

        return x

    def cross_entropy_loss(self, logits, labels):
        return F.nll_loss(logits, labels)

    def training_step(self, train_batch, batch_idx):
        x, y = train_batch
        logits = self.forward(x)
        loss = self.cross_entropy_loss(logits, y)
        self.log('train_loss', loss)
        return loss

    def validation_step(self, val_batch, batch_idx):
        x, y = val_batch
        logits = self.forward(x)
        loss = self.cross_entropy_loss(logits, y)
        self.log('val_loss', loss)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    def adversarial_attack(self, attack_fn):
        print(f"Running adversarial attack using {attack_fn.__name__} with epsilon={self.epsilon}")
        
        # Create a dataloader to fetch validation data
        data_loader = self.trainer.datamodule.val_dataloader()

        # Ensure the model is on the correct device
        device = self.device
        self.to(device)
        
        # Set model in evaluation mode
        self.eval()

        total_correct = 0
        total_samples = 0

        for batch in data_loader:
            x, y = batch

            # Move inputs and labels to the same device as the model
            x, y = x.to(device), y.to(device)

            # Generate adversarial examples using the passed attack function
            x_adv = attack_fn(self, x, y, self.epsilon)

            # Forward pass the adversarial examples to see how the model performs
            adv_logits = self.forward(x_adv)

            # Compute accuracy on adversarial examples
            preds = adv_logits.argmax(dim=1)
            total_correct += (preds == y).sum().item()
            total_samples += x.size(0)

        accuracy = total_correct / total_samples
        
        # Log the adversarial attack accuracy
        # self.logger.experiment('adversarial_attack_accuracy', accuracy)
        self.logger.experiment.add_scalar('adversarial_attack_accuracy', accuracy)
        
        print(f"Adversarial Attack Accuracy: {accuracy * 100:.2f}%")

    def on_train_end(self):
        # By default, pass FGSM as the attack function, but you can change it to other attacks
        self.adversarial_attack(fgsm_attack)
        
# Define the FGSM attack
def fgsm_attack(model, x, y, epsilon):
    """
    Perform FGSM attack
    :param model: The neural network model
    :param x: The input images
    :param y: The true labels
    :param epsilon: The perturbation size
    :return: Adversarial examples generated by FGSM
    """
    # Make sure x requires gradient
    x.requires_grad = True

    # Forward pass
    logits = model(x)
    
    # Compute the loss
    loss = F.nll_loss(logits, y)
    
    # Zero out previous gradients
    model.zero_grad()
    
    # Compute gradients of the loss w.r.t. the input image
    loss.backward()

    # Create adversarial examples using the sign of the gradients
    x_adv = x + epsilon * x.grad.sign()

    # Ensure the values are still within [0, 1]
    x_adv = torch.clamp(x_adv, 0, 1)
    
    return x_adv

class MNISTDataModule(pl.LightningDataModule):

  def setup(self, stage):
    # transforms for images
    transform=transforms.Compose([transforms.ToTensor(), 
                                  transforms.Normalize((0.1307,), (0.3081,))])
      
    # prepare transforms standard to MNIST
    self.mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)
    self.mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)

  def train_dataloader(self):
    return DataLoader(self.mnist_train, batch_size=64)

  def val_dataloader(self):
    return DataLoader(self.mnist_test, batch_size=64)


# Initialize data module
data_module = MNISTDataModule()

# Initialize model with a chosen epsilon for adversarial attacks
model = LightningMNISTClassifier(epsilon=0.25)

logger = pl.loggers.TensorBoardLogger("tb_logs", name="mnist_model")
csv_logger = pl.loggers.CSVLogger("csv_logs", name="mnist_model")

# Train the model
trainer = pl.Trainer(max_epochs=1, logger=logger)

# Training and the adversarial attack will automatically be called after training is done
trainer.fit(model, data_module)

# After training, call different attacks (can be done manually after training as well)
model.adversarial_attack(fgsm_attack)  # FGSM attack
# model.adversarial_attack(other_attack_fn)  # Another attack
